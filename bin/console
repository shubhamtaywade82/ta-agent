#!/usr/bin/env ruby
# frozen_string_literal: true

require "bundler/setup"

# Add lib to load path for development mode
lib_path = File.expand_path("../lib", __dir__)
$LOAD_PATH.unshift(lib_path) unless $LOAD_PATH.include?(lib_path)

require "ta-agent"

# Load LLM modules for easy access
require_relative "../lib/ta-agent/llm/ollama_client"
require_relative "../lib/ta-agent/llm/response_parser"
require_relative "../lib/ta-agent/agent/loop"
require_relative "../lib/ta-agent/agent/tool_registry"

# Helper methods for Ollama experimentation
module OllamaHelpers
  # Quick helper to create an Ollama client
  def ollama_client(host_url: nil, model: "llama3.2:3b")
    host_url ||= ENV["OLLAMA_HOST_URL"] || "http://192.168.1.14:11434"
    TaAgent::LLM::OllamaClient.new(host_url: host_url, model: model)
  end

  # Quick helper to send a simple chat message
  def chat(message, model: "llama3.2:3b", host_url: nil)
    client = ollama_client(host_url: host_url, model: model)
    response = client.chat(
      messages: [
        { role: "user", content: message }
      ]
    )

    if response && !response[:content].empty?
      puts response[:content]
    else
      puts "‚ö†Ô∏è  Empty response received"
      puts "Full response: #{response.inspect}"
      puts "Try: test_ollama(model: '#{model}') to debug"
    end
    response
  rescue TaAgent::OllamaError => e
    puts "‚ùå Error: #{e.message}"
    nil
  end

  # List available Ollama models
  def list_models(host_url: nil, debug: false)
    host_url ||= ENV["OLLAMA_HOST_URL"] || "http://192.168.1.14:11434"
    require "faraday"
    require "json"

    puts "üîç Fetching models from: #{host_url}/api/tags" if debug

    conn = Faraday.new(url: host_url) do |c|
      c.request :json
      c.response :json
    end

    response = conn.get("/api/tags")

    if debug
      puts "Response status: #{response.status}"
      puts "Response body: #{response.body.inspect}"
    end

    unless response.success?
      puts "‚ùå Failed to list models: HTTP #{response.status}"
      puts "Response: #{response.body.inspect}" if debug
      puts "\nüí° Make sure Ollama is running: ollama serve"
      return []
    end

    body = response.body
    if debug
      puts "Body type: #{body.class}"
      puts "Body keys: #{body.keys.inspect}" if body.is_a?(Hash)
    end

    # Handle different response formats
    models = if body.is_a?(Hash)
               body["models"] || body[:models] || []
             elsif body.is_a?(Array)
               body
             else
               []
             end

    if models.empty?
      puts "‚ö†Ô∏è  No models found on this Ollama server."
      puts "\nüí° To install a model, run in terminal:"
      puts "   ollama pull llama3.2:3b"
      puts "   ollama pull mistral"
      puts "   ollama pull llama2"
      puts "\nOr check if models exist: ollama list"
    else
      puts "üìã Available models (#{models.length}):"
      models.each do |model|
        # Handle both string and symbol keys
        name = model["name"] || model[:name] || model["model"] || model[:model] || "unknown"
        size = model["size"] || model[:size]

        size_str = size ? " (#{(size / 1024.0 / 1024.0 / 1024.0).round(2)} GB)" : ""
        puts "  ‚Ä¢ #{name}#{size_str}"
      end
    end

    # Return array of model names
    models.map do |m|
      m["name"] || m[:name] || m["model"] || m[:model]
    end.compact
  rescue Faraday::ConnectionFailed, Faraday::TimeoutError => e
    puts "‚ùå Connection failed: #{e.message}"
    puts "Make sure Ollama is running at #{host_url}"
    puts "Start it with: ollama serve"
    []
  rescue StandardError => e
    puts "‚ùå Error listing models: #{e.message}"
    puts e.backtrace.first(3).join("\n") if debug
    []
  end

  # Quick helper to test Ollama connection
  def test_ollama(host_url: nil, model: "llama3.2:3b", auto_find_model: true)
    host_url ||= ENV["OLLAMA_HOST_URL"] || "http://192.168.1.14:11434"
    puts "Testing Ollama connection..."
    puts "Host: #{host_url}"
    puts "Model: #{model}"

    begin
      client = ollama_client(host_url: host_url, model: model)
      response = client.chat(
        messages: [
          { role: "user", content: "Say 'Hello' if you can hear me." }
        ]
      )

      if response && !response[:content].empty?
        puts "‚úÖ Ollama connection successful!"
        puts "Response: #{response[:content]}"
        return response
      else
        puts "‚ö†Ô∏è  Ollama responded but content is empty"
        puts "Full response: #{response.inspect}"
      end
    rescue TaAgent::OllamaError => e
      error_msg = e.message
      puts "‚ùå Model '#{model}' not found: #{error_msg}"

      # Try to find an available model if auto_find_model is enabled
      if auto_find_model && error_msg.include?("not found")
        puts "\nüîç Searching for available models..."
        available_models = list_models(host_url: host_url, debug: false)

        if available_models.any?
          first_model = available_models.first
          puts "\n‚ú® Trying with first available model: #{first_model}"
          return test_ollama(host_url: host_url, model: first_model, auto_find_model: false)
        else
          puts "\nüí° No models found. Pull a model with:"
          puts "   ollama pull llama3.2:3b"
          puts "   ollama pull mistral"
          puts "   ollama pull llama2"
        end
      end

      puts "\nMake sure Ollama is running at #{host_url}"
      puts "Start Ollama with: ollama serve"
      return nil
    end

    nil
  end

  # Quick helper to create an agent loop
  def agent_loop(goal:, initial_context: {}, mode: :alert)
    config = TaAgent::Config.instance
    registry = TaAgent::Agent::ToolRegistry.new(mode: mode)
    TaAgent::Agent::Loop.new(
      goal: goal,
      initial_context: initial_context,
      tool_registry: registry,
      config: config
    )
  end

  # Debug helper to see raw Ollama response
  def debug_ollama_response(message: "Hello", model: "llama3.2:3b", host_url: nil)
    host_url ||= ENV["OLLAMA_HOST_URL"] || "http://192.168.1.14:11434"

    # Make raw request to see actual response
    require "faraday"
    require "json"
    conn = Faraday.new(url: host_url) do |c|
      c.request :json
      c.response :json
    end

    payload = {
      model: model,
      messages: [{ role: "user", content: message }],
      stream: false
    }

    puts "=" * 60
    puts "DEBUG: Ollama API Request"
    puts "=" * 60
    puts "URL: #{host_url}/api/chat"
    puts "Model: #{model}"
    puts "Payload:"
    puts JSON.pretty_generate(payload)
    puts "\n"

    response = conn.post("/api/chat") do |req|
      req.body = payload.to_json
      req.headers["Content-Type"] = "application/json"
    end

    puts "=" * 60
    puts "DEBUG: Raw Response"
    puts "=" * 60
    puts "Status: #{response.status}"
    puts "Body (raw):"
    puts JSON.pretty_generate(response.body)
    puts "\n"

    # Now test with the client
    puts "=" * 60
    puts "DEBUG: Parsed by OllamaClient"
    puts "=" * 60
    client = ollama_client(host_url: host_url, model: model)
    parsed = client.chat(messages: [{ role: "user", content: message }])
    puts parsed.inspect
    puts "\n"

    # Show structure analysis
    puts "=" * 60
    puts "DEBUG: Structure Analysis"
    puts "=" * 60
    body = response.body
    puts "Has 'message' key? #{body.key?("message") || body.key?(:message)}"
    if body["message"] || body[:message]
      msg = body["message"] || body[:message]
      puts "Message type: #{msg.class}"
      puts "Message keys: #{msg.keys.inspect}" if msg.is_a?(Hash)
      puts "Has 'content'? #{msg.key?("content") || msg.key?(:content)}" if msg.is_a?(Hash)
    end

    response.body
  end
end

# Include helpers in main scope
include OllamaHelpers

# Print welcome message with examples
puts <<~WELCOME
  üöÄ ta-agent Console - Ollama Integration Ready

  Quick Start:
  ------------
  # List available models
  list_models
  # Debug mode: list_models(debug: true)

  # Test Ollama connection (auto-finds model if needed)
  test_ollama

  # Simple chat
  chat("What is technical analysis?")

  # Create Ollama client
  client = ollama_client(model: "llama3.2:3b")
  response = client.chat(messages: [{ role: "user", content: "Hello!" }])

  # Create agent loop
  loop = agent_loop(goal: "Analyze NIFTY", initial_context: { symbol: "NIFTY" })
  result = loop.run

  Available Classes:
  ------------------
  - TaAgent::LLM::OllamaClient      # Direct Ollama API client
  - TaAgent::LLM::ResponseParser    # Parse LLM responses
  - TaAgent::Agent::Loop             # Agent loop (ReAct pattern)
  - TaAgent::Agent::ToolRegistry     # Tool registry for agent
  - TaAgent::Config                  # Configuration

  Type 'help' for more info or start experimenting!
WELCOME

require "irb"
IRB.start(__FILE__)
